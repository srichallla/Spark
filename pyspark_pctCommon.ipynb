{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_pctCommon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMMFd8j9SIbg3kf3nZTYhq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4bXcI0Ot9mC"
      },
      "source": [
        "# install Java8\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "# download spark3.0.0\r\n",
        "!wget -q wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\r\n",
        "# unzip it   spark-3.0.1-bin-hadoop2.7\r\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\r\n",
        "# install findspark \r\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPvwEC6-ukK2"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIAcYz0wusIT"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agy6Hooxy2x7"
      },
      "source": [
        "from pyspark import SparkConf, SparkContext, SQLContext\r\n",
        "from pyspark.sql.types import *\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from pyspark.sql.functions import *\r\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GnVjIar0adW"
      },
      "source": [
        "Using Spark DF API\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyKe2iFpzVbr",
        "outputId": "1e6e0118-8753-484d-ac31-3f0c1a80402c"
      },
      "source": [
        "df = spark.createDataFrame(    \r\n",
        "        [('hi how are you','how are you'), \r\n",
        "        ('who am i', 'who am i'),\r\n",
        "        ('this is human','this is'),\r\n",
        "        ('cow is the source of milk', 'bufallow is the source of milk'),\r\n",
        "        ('i am srinivas','i am srinivas')]\r\n",
        "    ,\r\n",
        "    ['c1', 'c2'] # add your columns label here\r\n",
        ")\r\n",
        "df.show()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|                  c1|                  c2|\n",
            "+--------------------+--------------------+\n",
            "|      hi how are you|         how are you|\n",
            "|            who am i|            who am i|\n",
            "|       this is human|             this is|\n",
            "|cow is the source...|bufallow is the s...|\n",
            "|       i am srinivas|       i am srinivas|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC5jLKYET5hG",
        "outputId": "4e128c6a-bc42-4ef6-e065-53d6ddf40492"
      },
      "source": [
        "def show_common_words(c1,c2):   \r\n",
        "    wc1 = c1.split(\" \") \r\n",
        "    wc2 = c2.split(\" \")\r\n",
        "    commonWords = set(wc1) & set(wc2)   \r\n",
        "    return commonWords \r\n",
        "\r\n",
        "def count_common_words(c1,c2):   \r\n",
        "    wc1 = c1.split(\" \") \r\n",
        "    wc2 = c2.split(\" \")\r\n",
        "    countOfCommonWords = len(set(wc1) & set(wc2))    \r\n",
        "    return countOfCommonWords      \r\n",
        "\r\n",
        "def compute_pct_similarity(c1,c2):   \r\n",
        "    wc1 = c1.split(\" \") \r\n",
        "    wc2 = c2.split(\" \")\r\n",
        "    pctOfCommonWords = (len(set(wc1) & set(wc2)) / len(wc1)) * 100     \r\n",
        "    return pctOfCommonWords      \r\n",
        "\r\n",
        "\r\n",
        "udf_func_showCommon = udf(show_common_words,StringType())\r\n",
        "udf_func_countCommon = udf(count_common_words,IntegerType())\r\n",
        "udf_func_pctCommon = udf(compute_pct_similarity,FloatType())\r\n",
        "\r\n",
        "#Create a new column in a datfarme using withcolumn\r\n",
        "df = df.withColumn(\"c3\",col(\"c1\") == col(\"c2\"))\r\n",
        "df = df.withColumn(\"c4\",udf_func_showCommon(df.c1, df.c2))\r\n",
        "df = df.withColumn(\"c5\",udf_func_countCommon(df.c1, df.c2))\r\n",
        "df = df.withColumn(\"c6\",udf_func_pctCommon(df.c1, df.c2))\r\n",
        "\r\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+--------------------+---+---------+\n",
            "|                  c1|                  c2|   c3|                  c4| c5|       c6|\n",
            "+--------------------+--------------------+-----+--------------------+---+---------+\n",
            "|      hi how are you|         how are you|false|     [how, are, you]|  3|     75.0|\n",
            "|            who am i|            who am i| true|        [i, am, who]|  3|    100.0|\n",
            "|       this is human|             this is|false|          [this, is]|  2|66.666664|\n",
            "|cow is the source...|bufallow is the s...|false|[the, of, milk, i...|  5|83.333336|\n",
            "|       I am srinivas|       I am srinivas| true|   [I, srinivas, am]|  3|    100.0|\n",
            "+--------------------+--------------------+-----+--------------------+---+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykH8UUW9Zw5r"
      },
      "source": [
        "Using Spark-SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfFXqhdxJJ8i",
        "outputId": "0ba9b396-0fb4-46f2-b59b-e55f902ca3c8"
      },
      "source": [
        "df.createOrReplaceTempView(\"sentencetable\")\r\n",
        "spark.sql(\"\"\" select c1,c2, filter(split(c1,' '), x -> array_contains(split(c2,' '),x) ) c4 from sentencetable \"\"\").withColumn(\"a1_size\",size('c4')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+-------+\n",
            "|                  c1|                  c2|                  c4|a1_size|\n",
            "+--------------------+--------------------+--------------------+-------+\n",
            "|      hi how are you|         how are you|     [how, are, you]|      3|\n",
            "|            who am i|            who am i|        [who, am, i]|      3|\n",
            "|       this is human|             this is|          [this, is]|      2|\n",
            "|cow is the source...|bufallow is the s...|[is, the, source,...|      5|\n",
            "|       i am srinivas|       i am srinivas|   [i, am, srinivas]|      3|\n",
            "+--------------------+--------------------+--------------------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}